{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input784\n",
      "<class 'numpy.ndarray'>\n",
      "[ 0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.45490196  0.49019608  0.67058825\n",
      "  1.          1.          0.58823532  0.36470589  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.66274512  0.99215686  0.99215686\n",
      "  0.99215686  0.99215686  0.99215686  0.99215686  0.85490197  0.11764706\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.66274512  0.99215686\n",
      "  0.99215686  0.99215686  0.83529413  0.55686277  0.6901961   0.99215686\n",
      "  0.99215686  0.47843137  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.20392157\n",
      "  0.98039216  0.99215686  0.82352942  0.1254902   0.04705882  0.\n",
      "  0.02352941  0.80784315  0.99215686  0.54901963  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.3019608   0.98431373  0.82352942  0.09803922  0.          0.          0.\n",
      "  0.47843137  0.97254902  0.99215686  0.25490198  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.12156863  0.07058824  0.          0.          0.          0.\n",
      "  0.81960785  0.99215686  0.99215686  0.25490198  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.45882353  0.96862745  0.99215686  0.7764706   0.03921569  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.29803923  0.96862745  0.99215686  0.90588236  0.24705882  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.50196081  0.99215686  0.99215686  0.56470591  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.6901961\n",
      "  0.96470588  0.99215686  0.62352943  0.04705882  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.09803922\n",
      "  0.91764706  0.99215686  0.9137255   0.13725491  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.7764706\n",
      "  0.99215686  0.99215686  0.5529412   0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.30588236\n",
      "  0.97254902  0.99215686  0.74117649  0.04705882  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.07450981\n",
      "  0.78431374  0.99215686  0.99215686  0.5529412   0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.52549022  0.99215686  0.99215686  0.67843139  0.04705882  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.97254902  0.99215686  0.99215686  0.09803922  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.97254902  0.99215686  0.99215686  0.16862746  0.07843138  0.07843138\n",
      "  0.07843138  0.07843138  0.01960784  0.          0.01960784  0.07843138\n",
      "  0.07843138  0.14509805  0.58823532  0.58823532  0.58823532  0.57647061\n",
      "  0.03921569  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.97254902  0.99215686  0.99215686\n",
      "  0.99215686  0.99215686  0.99215686  0.99215686  0.99215686  0.65882355\n",
      "  0.56078434  0.65098041  0.99215686  0.99215686  0.99215686  0.99215686\n",
      "  0.99215686  0.99215686  0.99215686  0.48235294  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.68235296  0.99215686  0.99215686  0.99215686  0.99215686  0.99215686\n",
      "  0.99215686  0.99215686  0.99215686  0.99215686  0.99215686  0.99215686\n",
      "  0.97647059  0.96862745  0.96862745  0.66274512  0.45882353  0.45882353\n",
      "  0.22352941  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.4627451   0.48235294\n",
      "  0.48235294  0.48235294  0.65098041  0.99215686  0.99215686  0.99215686\n",
      "  0.60784316  0.48235294  0.48235294  0.16078432  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "Layer-Wise Pretrain\n",
      "Layer 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration   main/loss   elapsed_time\n",
      "13          0.100006    0.874602      \n",
      "25          0.095632    2.12349       \n",
      "37          0.0851992   3.7697        \n",
      "49          0.069429    4.71991       \n",
      "62          0.0600351   6.60596       \n",
      "74          0.0593757   7.22251       \n",
      "86          0.0589908   8.04739       \n",
      "98          0.0573731   8.78076       \n",
      "111         0.0566976   9.47816       \n",
      "123         0.0563162   10.3285       \n",
      "135         0.0554592   11.1283       \n",
      "147         0.0537512   11.8538       \n",
      "160         0.0522248   12.7939       \n",
      "172         0.051917    13.6963       \n",
      "184         0.0499315   15.0347       \n",
      "196         0.0488576   16.4726       \n",
      "209         0.0467375   17.3196       \n",
      "221         0.0464233   18.5074       \n",
      "233         0.0449742   19.4561       \n",
      "245         0.0436392   20.5807       \n",
      "258         0.0432129   21.6534       \n",
      "270         0.0412647   22.3731       \n",
      "282         0.0412499   23.1145       \n",
      "294         0.0405571   24.1453       \n",
      "307         0.0396803   24.8417       \n",
      "319         0.0382881   26.0808       \n",
      "331         0.0389037   27.0968       \n",
      "343         0.0369607   27.7943       \n",
      "356         0.0372815   28.509        \n",
      "368         0.035859    29.367        \n",
      "380         0.0358234   30.3512       \n",
      "392         0.0353663   31.5869       \n",
      "405         0.0345954   33.3051       \n",
      "417         0.0347592   37.6334       \n",
      "429         0.0334656   38.6467       \n",
      "441         0.033607    40.4753       \n",
      "454         0.032549    41.7589       \n",
      "466         0.0325043   42.5158       \n",
      "478         0.0315832   43.2765       \n",
      "490         0.031582    44.1374       \n",
      "503         0.0306134   46.1316       \n",
      "515         0.0315444   48.7558       \n",
      "527         0.0298754   49.693        \n",
      "539         0.0301519   51.5131       \n",
      "552         0.0288481   53.1597       \n",
      "564         0.0291164   54.2588       \n",
      "576         0.0295585   55.8363       \n",
      "588         0.02819     56.6044       \n",
      "601         0.0281054   57.8287       \n",
      "613         0.0277587   58.7496       \n",
      "625         0.0272866   59.5073       \n",
      "637         0.0272787   60.1415       \n",
      "650         0.0268847   60.8685       \n",
      "662         0.0265108   61.5863       \n",
      "674         0.0260493   62.7274       \n",
      "686         0.025923    63.7739       \n",
      "699         0.0254514   64.9172       \n",
      "711         0.0253644   66.0244       \n",
      "723         0.0256709   67.1441       \n",
      "735         0.0246442   68.2462       \n",
      "748         0.0246432   69.1694       \n",
      "760         0.0243833   69.8798       \n",
      "772         0.0243468   70.6399       \n",
      "784         0.0238169   71.2808       \n",
      "797         0.0236403   72.0321       \n",
      "809         0.0240689   72.7408       \n",
      "821         0.023497    73.3538       \n",
      "833         0.0232846   74.0416       \n",
      "846         0.023204    74.749        \n",
      "858         0.022502    75.393        \n",
      "870         0.0228017   76.075        \n",
      "882         0.0223147   76.6869       \n",
      "895         0.022008    77.3759       \n",
      "907         0.0219787   78.2254       \n",
      "919         0.0218153   79.2901       \n",
      "931         0.0217972   80.4104       \n",
      "944         0.0215891   81.5597       \n",
      "956         0.0213102   82.4996       \n",
      "968         0.0213058   83.5745       \n",
      "980         0.0215012   84.7913       \n",
      "993         0.0210371   85.6004       \n",
      "1005        0.0205287   86.3492       \n",
      "1017        0.0208873   87.173        \n",
      "1029        0.0205262   87.7848       \n",
      "1042        0.0201987   88.5025       \n",
      "1054        0.0204956   89.2592       \n",
      "1066        0.0203813   89.8948       \n",
      "1078        0.0199607   90.6092       \n",
      "1091        0.0198582   91.3155       \n",
      "1103        0.0194672   91.991        \n",
      "1115        0.0193882   92.6795       \n",
      "1127        0.0194513   93.3171       \n",
      "1140        0.0194129   94.3131       \n",
      "1152        0.0189534   95.4358       \n",
      "1164        0.0186783   96.5242       \n",
      "1176        0.0190168   97.5854       \n",
      "1189        0.0184668   98.9425       \n",
      "1201        0.0188362   99.9219       \n",
      "1213        0.0186293   100.995       \n",
      "1225        0.0185107   101.779       \n",
      "1238        0.0183567   102.614       \n",
      "1250        0.0181863   103.257       \n",
      "1262        0.018007    103.93        \n",
      "1274        0.0180575   104.784       \n",
      "1287        0.0179271   105.55        \n",
      "1299        0.0178458   106.235       \n",
      "1311        0.0177523   106.98        \n",
      "1323        0.0177402   107.681       \n",
      "1336        0.0174544   108.524       \n",
      "1348        0.0177744   109.224       \n",
      "1360        0.0169375   110.001       \n",
      "1372        0.0172146   111.359       \n",
      "1385        0.0171878   112.337       \n",
      "1397        0.0169513   113.638       \n",
      "1409        0.0170799   115.202       \n",
      "1421        0.0171725   116.234       \n",
      "1434        0.016872    117.265       \n",
      "1446        0.0165254   118.086       \n",
      "1458        0.01671     118.832       \n",
      "1470        0.0166498   119.744       \n",
      "1483        0.0160987   120.854       \n",
      "1495        0.0166014   122.643       \n",
      "1507        0.0163293   123.754       \n",
      "1519        0.0162814   126.096       \n",
      "1532        0.0165171   128.108       \n",
      "1544        0.0161987   129.86        \n",
      "1556        0.0157971   133.786       \n",
      "1568        0.0160978   137.432       \n",
      "1581        0.0158733   139.113       \n",
      "1593        0.0159839   141.646       \n",
      "1605        0.0158768   144.624       \n",
      "1617        0.0157327   145.964       \n",
      "1630        0.0157938   149.556       \n",
      "1642        0.0152975   151.241       \n",
      "1654        0.0156157   154.379       \n",
      "1666        0.0152476   155.115       \n",
      "1679        0.0154272   155.953       \n",
      "1691        0.0152225   156.718       \n",
      "1703        0.0152062   157.718       \n",
      "1715        0.0151924   161.183       \n",
      "1728        0.0148635   163.858       \n",
      "1740        0.0148877   164.876       \n",
      "1752        0.0147261   166.02        \n",
      "1764        0.0148322   167.093       \n",
      "1777        0.0150237   168.428       \n",
      "1789        0.0148921   169.245       \n",
      "1801        0.0149093   170.086       \n",
      "1813        0.0146927   170.921       \n",
      "1826        0.0147174   172.418       \n",
      "1838        0.0144463   173.212       \n",
      "1850        0.0144339   174.178       \n",
      "1862        0.0145297   175.276       \n",
      "1875        0.014372    176.387       \n",
      "1887        0.0141646   177.468       \n",
      "1899        0.0144372   178.674       \n",
      "1911        0.0140042   179.712       \n",
      "1924        0.0143179   180.813       \n",
      "1936        0.0138672   181.662       \n",
      "1948        0.0139239   182.524       \n",
      "1960        0.0140053   183.317       \n",
      "1973        0.014017    184.162       \n",
      "1985        0.0138913   184.851       \n",
      "1997        0.0135587   185.792       \n",
      "2009        0.0136534   186.944       \n",
      "2022        0.0136737   187.834       \n",
      "2034        0.0138464   188.943       \n",
      "2046        0.0139377   189.78        \n",
      "2058        0.0140039   191.167       \n",
      "2071        0.0132324   196.707       \n",
      "2083        0.0135814   202.798       \n",
      "2095        0.0130911   205.245       \n",
      "2107        0.0134601   207.751       \n",
      "2120        0.013445    208.691       \n",
      "2132        0.0136427   210.05        \n",
      "2144        0.0132583   211.715       \n",
      "2156        0.013189    213.054       \n",
      "2169        0.0129854   213.96        \n",
      "2181        0.0133265   214.773       \n",
      "2193        0.0130368   215.79        \n",
      "2205        0.0132428   216.525       \n",
      "2218        0.012865    217.275       \n",
      "2230        0.0128832   217.898       \n",
      "2242        0.0128766   218.554       \n",
      "2254        0.0132189   219.523       \n",
      "2267        0.0128671   220.234       \n",
      "2279        0.0129586   221.011       \n",
      "2291        0.0129287   223.283       \n",
      "2303        0.0129057   224.883       \n",
      "2316        0.0127171   227.206       \n",
      "2328        0.0127316   228.485       \n",
      "2340        0.0129359   230.342       \n",
      "2352        0.0125417   231.717       \n",
      "2365        0.0124414   232.565       \n",
      "2377        0.0123892   233.511       \n",
      "2389        0.0123654   234.37        \n",
      "2401        0.0123104   235.217       \n",
      "2414        0.0124518   236.172       \n",
      "2426        0.0123403   237.005       \n",
      "2438        0.01237     237.821       \n",
      "2450        0.0124045   240.86        \n",
      "2463        0.0120941   245.351       \n",
      "2475        0.0120704   246.382       \n",
      "2487        0.0121197   247.492       \n",
      "2499        0.0120243   248.242       \n",
      "2512        0.0119503   249.082       \n",
      "2524        0.0119688   249.798       \n",
      "2536        0.0117696   250.55        \n",
      "2548        0.012139    251.358       \n",
      "2561        0.0118083   252.149       \n",
      "2573        0.0120188   252.927       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2585        0.0120418   253.724       \n",
      "2597        0.0117554   254.764       \n",
      "2610        0.0117271   255.891       \n",
      "2622        0.011747    257.052       \n",
      "2634        0.0116618   258.209       \n",
      "2646        0.0119844   259.343       \n",
      "2659        0.0116453   260.646       \n",
      "2671        0.0114728   261.474       \n",
      "2683        0.011409    262.284       \n",
      "2695        0.0114749   263.009       \n",
      "2708        0.0115694   263.743       \n",
      "2720        0.0118195   264.457       \n",
      "2732        0.0116942   265.144       \n",
      "2744        0.0114221   265.838       \n",
      "2757        0.0116122   266.613       \n",
      "2769        0.0114431   267.379       \n",
      "2781        0.0114196   268.087       \n",
      "2793        0.0112426   268.802       \n",
      "2806        0.011181    269.686       \n",
      "2818        0.0112829   270.761       \n",
      "2830        0.010895    272.116       \n",
      "2842        0.0112097   273.627       \n",
      "2855        0.0113251   275.821       \n",
      "2867        0.0109756   277.247       \n",
      "2879        0.0110692   278.631       \n",
      "2891        0.0110922   279.485       \n",
      "2904        0.010818    281.049       \n",
      "2916        0.0112114   281.655       \n",
      "2928        0.0109061   282.556       \n",
      "2940        0.0110016   283.289       \n",
      "2953        0.0108483   284.333       \n",
      "2965        0.0109399   285.219       \n",
      "2977        0.0108736   286.013       \n",
      "2989        0.0111512   287.222       \n",
      "3002        0.0104945   288.412       \n",
      "3014        0.0106034   289.6         \n",
      "3026        0.011021    290.706       \n",
      "3038        0.0107207   291.778       \n",
      "3051        0.010756    292.82        \n",
      "3063        0.0106606   293.503       \n",
      "3075        0.0106196   294.339       \n",
      "3087        0.0101927   295.033       \n",
      "3100        0.0106807   295.741       \n",
      "3112        0.0106692   296.448       \n",
      "3124        0.0106132   297.155       \n",
      "3136        0.010445    297.832       \n",
      "3149        0.0106433   298.588       \n",
      "3161        0.0104373   299.294       \n",
      "3173        0.0106086   300.022       \n",
      "3185        0.010288    300.7         \n",
      "3198        0.0102714   301.434       \n",
      "3210        0.0102841   302.933       \n",
      "3222        0.0100889   304.036       \n",
      "3234        0.0103574   305.188       \n",
      "3247        0.0103824   306.379       \n",
      "3259        0.0101563   307.464       \n",
      "3271        0.0101837   308.543       \n",
      "3283        0.0105043   309.523       \n",
      "3296        0.0101292   310.282       \n",
      "3308        0.0101727   311.14        \n",
      "3320        0.0103693   311.896       \n",
      "3332        0.0101081   312.62        \n",
      "3345        0.00999936  313.328       \n",
      "3357        0.0100571   314.036       \n",
      "3369        0.00993694  314.765       \n",
      "3381        0.00986551  315.448       \n",
      "3394        0.0101109   316.181       \n",
      "3406        0.0100495   316.947       \n",
      "3418        0.00977183  317.633       \n",
      "3430        0.00960688  318.604       \n",
      "3443        0.00983759  319.772       \n",
      "3455        0.00993987  320.921       \n",
      "3467        0.00986846  321.997       \n",
      "3479        0.0099641   323.105       \n",
      "3492        0.0100436   324.128       \n",
      "3504        0.00993794  325.238       \n",
      "3516        0.00973826  325.939       \n",
      "3528        0.0102316   326.752       \n",
      "3541        0.00970476  327.514       \n",
      "3553        0.0097301   328.199       \n",
      "3565        0.00962331  328.875       \n",
      "3577        0.00979344  329.676       \n",
      "3590        0.00971535  330.511       \n",
      "3602        0.00969146  331.373       \n",
      "3614        0.00974719  332.065       \n",
      "3626        0.00974587  332.797       \n",
      "3639        0.0095202   333.547       \n",
      "3651        0.009848    334.387       \n",
      "3663        0.00951811  335.636       \n",
      "3675        0.00941161  336.671       \n",
      "3688        0.00942592  337.865       \n",
      "3700        0.00936839  338.877       \n",
      "3712        0.00933463  340.141       \n",
      "3724        0.0096952   341.487       \n",
      "3737        0.00946198  342.294       \n",
      "3749        0.00928351  342.971       \n",
      "3761        0.00930492  343.679       \n",
      "3773        0.00940193  344.353       \n",
      "3786        0.00945415  345.093       \n",
      "3798        0.00917064  346.157       \n",
      "3810        0.00934526  347.26        \n",
      "3822        0.00950168  348.17        \n",
      "3835        0.00921119  348.903       \n",
      "3847        0.00906316  349.641       \n",
      "3859        0.00898411  350.667       \n",
      "3871        0.00913746  351.78        \n",
      "3884        0.00921913  352.983       \n",
      "3896        0.00910677  354.063       \n",
      "3908        0.00918738  355.017       \n",
      "3920        0.0092073   356.117       \n",
      "3933        0.00924378  357.205       \n",
      "3945        0.00900946  357.999       \n",
      "3957        0.00873802  358.706       \n",
      "3969        0.00907978  359.452       \n",
      "3982        0.00911362  360.251       \n",
      "3994        0.00914088  360.905       \n",
      "4006        0.00930768  361.615       \n",
      "4018        0.00864193  362.517       \n",
      "4031        0.00913188  363.414       \n",
      "4043        0.00902139  364.116       \n",
      "4055        0.00885545  364.828       \n",
      "4067        0.0089577   365.537       \n",
      "4080        0.00899164  366.494       \n",
      "4092        0.00888684  367.7         \n",
      "4104        0.00889819  368.621       \n",
      "4116        0.00862092  369.688       \n",
      "4129        0.00840075  370.823       \n",
      "4141        0.00866125  371.909       \n",
      "4153        0.00848224  372.999       \n",
      "4165        0.00871894  373.704       \n",
      "4178        0.00879837  374.556       \n",
      "4190        0.00873388  375.272       \n",
      "4202        0.00867937  375.99        \n",
      "4214        0.00849333  377.171       \n",
      "4227        0.00893877  378.3         \n",
      "4239        0.00843455  379.351       \n",
      "4251        0.00886698  380.253       \n",
      "4263        0.0088454   381.131       \n",
      "4276        0.00875079  381.852       \n",
      "4288        0.00883933  383.042       \n",
      "4300        0.0085088   384.173       \n",
      "4312        0.00842331  385.131       \n",
      "4325        0.00855432  386.266       \n",
      "4337        0.00884001  387.35        \n",
      "4349        0.0084358   388.48        \n",
      "4361        0.0085315   389.468       \n",
      "4374        0.00867859  390.294       \n",
      "4386        0.00848956  391.027       \n",
      "4398        0.00833318  391.722       \n",
      "4410        0.0083502   392.434       \n",
      "4423        0.00869552  393.183       \n",
      "4435        0.0085099   394.523       \n",
      "4447        0.0082259   395.368       \n",
      "4459        0.00821019  397.193       \n",
      "4472        0.00833681  400.579       \n",
      "4484        0.00845606  402.629       \n",
      "4496        0.00839519  405.959       \n",
      "4508        0.0084359   407.4         \n",
      "4521        0.00818712  408.659       \n",
      "4533        0.00838409  410.036       \n",
      "4545        0.00831185  411.162       \n",
      "4557        0.00815868  412.285       \n",
      "4570        0.0082087   413.434       \n",
      "4582        0.00840357  414.606       \n",
      "4594        0.00829805  416.2         \n",
      "4606        0.0083948   417.646       \n",
      "4619        0.00818231  418.941       \n",
      "4631        0.00809603  420.125       \n",
      "4643        0.00856826  421.239       \n",
      "4655        0.00817158  422.124       \n",
      "4668        0.00841908  423.128       \n",
      "4680        0.00789263  423.872       \n",
      "4692        0.0081942   424.739       \n",
      "4704        0.00813529  425.679       \n",
      "4717        0.00808066  426.692       \n",
      "4729        0.00815699  427.6         \n",
      "4741        0.00802774  428.633       \n",
      "4753        0.00813769  429.859       \n",
      "4766        0.00818422  431.371       \n",
      "4778        0.00769539  433.023       \n",
      "4790        0.00818079  434.257       \n",
      "4802        0.00815228  435.061       \n",
      "4815        0.00792655  436.405       \n",
      "4827        0.00812756  437.647       \n",
      "4839        0.00781227  438.774       \n",
      "4851        0.00802451  440.612       \n",
      "4864        0.00801129  441.561       \n",
      "4876        0.00780194  443.552       \n",
      "4888        0.00801312  444.491       \n",
      "4900        0.00774572  445.565       \n",
      "4913        0.00791719  446.914       \n",
      "4925        0.00800965  448.216       \n",
      "4937        0.00806362  449.564       \n",
      "4949        0.00813777  453.293       \n",
      "4962        0.00775296  454.025       \n",
      "4974        0.00771648  454.992       \n",
      "4986        0.00802023  456.433       \n",
      "4998        0.00763424  457.398       \n",
      "5011        0.00795521  458.73        \n",
      "5023        0.00786257  459.581       \n",
      "5035        0.00759149  460.778       \n",
      "5047        0.00790598  462.364       \n",
      "5060        0.00771292  463.748       \n",
      "5072        0.00774134  464.911       \n",
      "5084        0.00784392  466.029       \n",
      "5096        0.00738063  467.373       \n",
      "5109        0.00794666  468.553       \n",
      "5121        0.00792812  469.544       \n",
      "5133        0.0074063   470.269       \n",
      "5145        0.0075271   471.439       \n",
      "5158        0.00768738  472.575       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5170        0.00755803  473.884       \n",
      "5182        0.00769503  475.337       \n",
      "5194        0.00779944  477.425       \n",
      "5207        0.00752069  478.952       \n",
      "5219        0.00778165  480.561       \n",
      "5231        0.00772543  482.037       \n",
      "5243        0.00760376  484.694       \n",
      "5256        0.00756777  487.342       \n",
      "5268        0.00769016  488.924       \n",
      "5280        0.00759321  489.871       \n",
      "5292        0.00770871  491.861       \n",
      "5305        0.00737412  493.484       \n",
      "5317        0.00758107  497.123       \n",
      "5329        0.00757588  498.986       \n",
      "5341        0.00762888  500.988       \n",
      "5354        0.00748852  502.411       \n",
      "5366        0.00732151  504.873       \n",
      "5378        0.00750416  505.816       \n",
      "5390        0.00741478  507.057       \n",
      "5403        0.00751778  508.383       \n",
      "5415        0.00766488  509.294       \n",
      "5427        0.00734026  511.987       \n",
      "5439        0.0075408   517.26        \n"
     ]
    }
   ],
   "source": [
    "# coding:utf-8\n",
    "import argparse\n",
    "\n",
    "import chainer\n",
    "import chainer.links as L\n",
    "from chainer.datasets import mnist, tuple_dataset\n",
    "from chainer import iterators\n",
    "from chainer import optimizers\n",
    "from chainer import training\n",
    "from chainer import serializers\n",
    "from chainer.training import extensions\n",
    "from chainer.dataset import convert\n",
    "from chainer.functions.loss.mean_squared_error import mean_squared_error\n",
    "#import cupy as cp\n",
    "import numpy as np\n",
    "\n",
    "from change_learning_rate import ChangeLearningRate\n",
    "from stacked_denoising_autoencoder import StackedDenoisingAutoEncoder\n",
    "\n",
    "import glob, os\n",
    "from PIL import Image\n",
    "import easydict\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\"\"\"\n",
    "事前学習は2つのステップに分かれており、\n",
    "　・1層毎にAutoEncoderとして学習を行う「Layer-Wise Pretrain」\n",
    "　・ネットワーク全体を学習する「Fine Tuning」\n",
    "\"\"\"\n",
    "\n",
    "def load_image():\n",
    "    x = 28\n",
    "    y = x\n",
    "    filepaths = glob.glob('test/*.png')\n",
    "    f = open('text.txt', 'w') # 追記モードで開く    \n",
    "    f.close()\n",
    "    \n",
    "    datasets = []\n",
    "    i = 0\n",
    "    \n",
    "    for filepath in filepaths:\n",
    "        basename = os.path.basename(filepath).split('.')[0]\n",
    "        img = Image.open(filepath).convert('L')  #Pillowで読み込み。'L'はグレースケール\n",
    "        #img = img.resize((x, y)) # 32x32xにリサイズ\n",
    "\n",
    "        data = np.array(np.float32(img)/255.0)\n",
    "        data = data.reshape(x * y) # 1行に変換\n",
    "        t = np.array(i, dtype=np.int32) \n",
    "        \n",
    "        datasets.append((data, t)) # データとラベルをタプルでリストに入れる\n",
    "\n",
    "        f = open('text.txt', 'a') # 追記モードで開く\n",
    "        f.write(str(i) + ', ' + basename + '\\n') # 引数の文字列をファイルに書き込む\n",
    "        f.close()\n",
    "        \n",
    "        i += 1\n",
    "#     random.shuffle(datasets) # シャッフル\n",
    "#     train = datasets[:1000] # 最初の千個を学習用\n",
    "#     test = datasets[1000:1100] # 千個めから1100個目までをテスト用\n",
    "\n",
    "    return datasets, None # 学習用、テスト用に分けず作成したデータセットごと返す\n",
    "\n",
    "def pretraining():\n",
    "#     parser = argparse.ArgumentParser()\n",
    "#     parser.add_argument('--gpu', type=int, default=-1)\n",
    "#     parser.add_argument('--seed', type=int, default=0)\n",
    "#     parser.add_argument('--batchsize', type=int, default=256)\n",
    "#     args = parser.parse_args()\n",
    "\n",
    "    args = easydict.EasyDict({\n",
    "        \"gpu\": -1,\n",
    "        \"seed\": 0,\n",
    "        \"model_seed\": 0,\n",
    "        \"batchsize\": 8\n",
    "    }) \n",
    "\n",
    "    xp = np\n",
    "    gpu_id = args.gpu\n",
    "    seed = args.seed\n",
    "    train, _ =  load_image() # mnist.get_mnist()\n",
    "    \n",
    "    train, _ = convert.concat_examples(train, device=gpu_id)\n",
    "    print('input' + str(train.shape[1]))\n",
    "    batchsize = args.batchsize\n",
    "    model = StackedDenoisingAutoEncoder(input_dim=train.shape[1])\n",
    "    if chainer.cuda.available and args.gpu >= 0:\n",
    "        xp = cp\n",
    "        model.to_gpu(gpu_id)\n",
    "    xp.random.seed(seed)\n",
    "    print(type(train))\n",
    "    print(train[1])\n",
    "\n",
    "    # Layer-Wise Pretrain\n",
    "    # 各層を取得しAutoEncoderとして学習する\n",
    "    print(\"Layer-Wise Pretrain\")\n",
    "    for i, dae in enumerate(model.children()):\n",
    "        print(\"Layer {}\".format(i+1))\n",
    "        train_tuple = tuple_dataset.TupleDataset(train, train)\n",
    "        train_iter = iterators.SerialIterator(train_tuple, batchsize)\n",
    "        clf = L.Classifier(dae, lossfun=mean_squared_error)\n",
    "        clf.compute_accuracy = False\n",
    "        if chainer.cuda.available and args.gpu >= 0:\n",
    "            clf.to_gpu(gpu_id)\n",
    "        optimizer = optimizers.MomentumSGD(lr=0.1)\n",
    "        optimizer.setup(clf)\n",
    "        updater = training.StandardUpdater(train_iter, optimizer, device=gpu_id)\n",
    "        trainer = training.Trainer(updater, (50000, \"iteration\"), out=\"mnist_result\")\n",
    "        trainer.extend(extensions.LogReport())\n",
    "        trainer.extend(extensions.PrintReport(['iteration', 'main/loss', 'elapsed_time']))\n",
    "        # 20000iteration毎に学習率(lr)を1/10にする\n",
    "        trainer.extend(ChangeLearningRate(), trigger=(20000, \"iteration\"))\n",
    "        trainer.run()\n",
    "        # 層毎の学習に合わせて学習データの次元を784->500->500->2000と変換していく\n",
    "        train = dae.encode(train).data\n",
    "\n",
    "    # Finetuning\n",
    "    print(\"fine tuning\")\n",
    "    # Fine Tuning時はDropoutなし\n",
    "    with chainer.using_config(\"train\", False):\n",
    "        train, _ = mnist.get_mnist()\n",
    "        train, _ = convert.concat_examples(train, device=gpu_id)\n",
    "        train_tuple = tuple_dataset.TupleDataset(train, train)\n",
    "        train_iter = iterators.SerialIterator(train_tuple, batchsize)\n",
    "        model = L.Classifier(model, lossfun=mean_squared_error)\n",
    "        model.compute_accuracy = False\n",
    "        if chainer.cuda.available and args.gpu >= 0:\n",
    "            model.to_gpu(gpu_id)\n",
    "        optimizer = optimizers.MomentumSGD(lr=0.1)\n",
    "        optimizer.setup(model)\n",
    "        updater = training.StandardUpdater(train_iter, optimizer, device=gpu_id)\n",
    "        trainer = training.Trainer(updater, (100000, \"iteration\"), out=\"mnist_result\")\n",
    "        trainer.extend(extensions.LogReport())\n",
    "        trainer.extend(extensions.PrintReport(['iteration', 'main/loss', 'elapsed_time']))\n",
    "        trainer.extend(ChangeLearningRate(), trigger=(20000, \"iteration\"))\n",
    "        trainer.run()\n",
    "\n",
    "    outfile = \"StackedDenoisingAutoEncoder-seed{}.model\".format(seed)\n",
    "    serializers.save_npz(outfile, model.predictor)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    pretraining()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = 256\n",
    "cols = rows\n",
    "\n",
    "img = Image.open('whale/images.jpg').convert('L')  #Pillowで読み込み。'L'はグレースケールを意味する\n",
    "img = img.resize((rows, cols)) # 32x32xにリサイズ\n",
    "label = 0 # ラベル(ファイル名)\n",
    "\n",
    "x = np.array(img, dtype=np.float32)\n",
    "x = x.reshape(rows*cols)\n",
    "\n",
    "print(x)\n",
    "# x = x.reshape(1, 32, 32) # (チャネル、高さ、横幅)\n",
    "# t = np.array(label, dtype=np.int32) \n",
    "hoge = train[0][0].reshape(28, 28)\n",
    "plt.imshow(train[0][0].reshape(28, 28), cmap='gray')\n",
    "plt.imshow(x.reshape(rows, cols),cmap='gray')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
